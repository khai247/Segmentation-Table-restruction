{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegTable.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcUTa7pBJ_y0",
        "outputId": "65900e0a-5239-4031-f4cd-e91aaa5a5bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import statistics\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time"
      ],
      "metadata": {
        "id": "eGpN4ES9ewGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "!pip install albumentations==0.4.6"
      ],
      "metadata": {
        "id": "_G9HOOpnfbn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b32c15-a331-4ab8-ddb0-4a364d45d8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.2.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=82603317883cfa4454a45ebcda2a3fd7107c74886714b849efac1142573b6a5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(img_h, img_w, label_path,LINE_HEIGHT_RATIO):\n",
        "    #line_mask = np.zeros((img_h, img_w))\n",
        "    table_mask = np.zeros((img_h, img_w))\n",
        "    with open(label_path, 'r') as f:\n",
        "\n",
        "        label_lines = f.readlines()\n",
        "        # draw mask\n",
        "        for line in label_lines:\n",
        "            line_content = line.strip().split(' ') if '[' not in line else ['[' + a for a in line.strip().split('[')]\n",
        "            line_type = line_content[0]\n",
        "            if 'table' in line_type:\n",
        "                if len(line_content) == 3:  # type, x_points, y_points\n",
        "                    if ', ' in line_content[1].strip()[1:-1]:\n",
        "                        sep = ', '\n",
        "                    else:\n",
        "                        sep = ' '\n",
        "                    x_points, y_points = np.fromstring(line_content[1].strip()[1:-1], sep=sep).astype(\n",
        "                        np.int32), np.fromstring(line_content[2].strip()[1:-1], sep=sep).astype(np.int32)\n",
        "                    #print(x_points,y_points)\n",
        "                    if x_points.shape[0] == 2:\n",
        "                        #print(x_points[0])\n",
        "                        #print(y_points[0])\n",
        "                        x_points_new = [x_points[0], y_points[0]]\n",
        "                        y_points_new = [x_points[1], y_points[1]]\n",
        "                        cv2.polylines(table_mask, [np.stack([x_points_new, y_points_new], axis=1)], True, 1, 6)\n",
        "\n",
        "                    \"\"\"else:\n",
        "                        cv2.polylines(table_mask, [np.stack([x_points, y_points], axis=1)], True, 255, 2)\n",
        "                        print(1)\"\"\"\n",
        "                \"\"\"if len(line_content) == 2:\n",
        "                    new_list = re.findall('[0-9]+', line_content[1])\n",
        "                    x, y, w, h = int(new_list[0]), int(new_list[1]), int(new_list[2]), int(new_list[3])\n",
        "                    cv2.rectangle(table_mask, (x, y), (w, h), 255, 2)\"\"\"\n",
        "\n",
        "    return table_mask"
      ],
      "metadata": {
        "id": "5Mbj3vRpg98H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        #mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "        mask_path = img_path.replace(\"img\", \"label\").replace(\"png\", \"txt\")\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        img_h, img_w, _ = image.shape\n",
        "\n",
        "        mask = create_mask(img_h=img_h,img_w=img_w,label_path=mask_path,LINE_HEIGHT_RATIO=0.5)\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations['image']\n",
        "            mask = augmentations['mask']\n",
        "        image = image[:,7:-7,7:-7]     \n",
        "        mask = mask[7:-7,7:-7]\n",
        "\n",
        "\n",
        "        #assert image.shape[:2] == mask.shape[:2]\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "NRETjub2h4vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter data\n",
        "pin_memory = True\n",
        "batch_size = 1\n",
        "num_workers = 2\n",
        "image_height = 512\n",
        "image_width = 512\n",
        "train_img_dir = \"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/img_train\"\n",
        "train_mask_dir = \"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/label_train\"\n",
        "val_img_dir = \"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/img_val\"\n",
        "val_mask_dir = \"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/label_val\"\n",
        "test_mask_dir = \"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/label_test\"\n",
        "test_img_dir = \"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/img_test\"\n",
        "\n",
        "\n",
        "train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=image_height, width=image_width,interpolation=cv2.INTER_NEAREST),\n",
        "            A.Rotate(limit=90, p=1.0,interpolation=cv2.INTER_NEAREST),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.IAAAdditiveGaussianNoise (loc=0, scale=(0.01, 0.05), per_channel=False, always_apply=False, p=0.5),\n",
        "            A.GaussNoise(),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "val_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=image_height, width=image_width,interpolation=cv2.INTER_NEAREST),\n",
        "            A.Normalize(),\n",
        "            ToTensorV2(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def get_loader(\n",
        "            train_dir,\n",
        "            train_maskdir,\n",
        "            val_dir,\n",
        "            val_maskdir,\n",
        "            batch_size,\n",
        "            train_transform=True,\n",
        "            val_transform=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True):\n",
        "    train_ds = SegmentDataset(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform= train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_ds = SegmentDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    return train_loader,val_loader\n",
        "\n",
        "train_loader, val_loader = get_loader(\n",
        "        train_img_dir,\n",
        "        train_mask_dir,\n",
        "        val_img_dir,\n",
        "        val_mask_dir,\n",
        "        batch_size,\n",
        "        train_transform,\n",
        "        val_transform,\n",
        "        num_workers,\n",
        "        pin_memory\n",
        "    )"
      ],
      "metadata": {
        "id": "C3Ic7_O3g9_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.dropout import Dropout\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels,out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.25)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Down part\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        # Up part\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.ups.append(DoubleConv(feature * 2, feature))\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for index in range(0, len(self.ups), 2):\n",
        "            x = self.ups[index](x)\n",
        "            skip_connection = skip_connections[index // 2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[index + 1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "f4RKtb-rhXoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/my_checkpoint_table_10.pth\"):\n",
        "    print(\"=>Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=>Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder=\"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/save_img_train\", device=\"cuda\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "       \n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "        torchvision.utils.save_image(\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\n",
        "        )\n",
        "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n"
      ],
      "metadata": {
        "id": "DZECelfvgNTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_epochs = 100\n",
        "load_model = True\n",
        "model = UNet(in_channels=3,out_channels=1).to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "Ekw8Xzoiizxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader, model, optimizer, loss_fn):\n",
        "    writer = SummaryWriter('runs/seg_table')\n",
        "    loop = tqdm(loader)\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    tsb = 0\n",
        "    for batch_index,(data,targets) in enumerate(loop):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.float().unsqueeze(1).to(device=device)\n",
        "        #forward\n",
        "        #start_time = time.time()\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        \n",
        "        #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "        #backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #update tqdm loop\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_correct += (predictions == targets).sum().item()\n",
        "        if tsb % 100 == 0:    # every 100 mini-batches...\n",
        "\n",
        "            # ...log the running loss\n",
        "            writer.add_scalar('training loss',\n",
        "                            running_loss / 100,\n",
        "                            epoch * len(train_loader) + batch_index)\n",
        "            writer.add_scalar('val loss',\n",
        "                            running_loss / 100,\n",
        "                            epoch * len(val_loader) + batch_index)\n",
        "            writer.add_scalar('training accuracy',\n",
        "                            running_correct/len(train_loader), batch_index)\n",
        "            writer.add_scalar('val accuracy',\n",
        "                            running_correct/len(val_loader), batch_index)             \n",
        "            \"\"\"writer.add_scalar('training dice_coeff',\n",
        "                            running_loss / 100,\n",
        "                            epoch * len(train_loader) + batch_index)\n",
        "            writer.add_scalar('val dice_coeff',\n",
        "                            running_loss / 100,\n",
        "                            epoch * len(train_loader) + batch_index)\"\"\"\n",
        "            running_loss = 0  \n",
        "        tsb = tsb + 1  \n",
        "        \n"
      ],
      "metadata": {
        "id": "aRKgPLk5gNa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coeff(loader,model,device):\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data,target in loader:\n",
        "            data = data.to(device=device)\n",
        "            target = target.float().unsqueeze(1).to(device=device).detach().cpu().numpy()\n",
        "            #img_path = img_path[0]\n",
        "            #img_name = img_path.rsplit('/', 1)[-1]\n",
        "            #image = Image.open(img_path)\n",
        "            pred = torch.sigmoid(model(data))\n",
        "            #print(pred.shape) #(4,1,630,630)\n",
        "            pred = (pred > 0.5).float().detach().cpu().numpy()\n",
        "            #pred = pred.reshape(512, 512).detach().cpu().numpy()\n",
        "            #image = np.array(image)\n",
        "            #image = cv2.resize(image,(512,512))\n",
        "            #print(image.shape)\n",
        "            #image[pred == 1] = (0, 255, 0)\n",
        "\n",
        "            #cv2.imwrite('/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/vis/' + img_name,image)\n",
        "            smooth = 1e-8\n",
        "            intersection = (pred * target).sum()\n",
        "            dice_score += ((2. * intersection) + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "    #print(len(loader))\n",
        "    print(f\"Dice score:{dice_score/len(loader)}\")"
      ],
      "metadata": {
        "id": "I7x1jP1Njk-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_checkpoint(torch.load(\"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/my_checkpoint_table_10.pth\",map_location=torch.device('cpu')),model)    \n",
        "dice_coeff(val_loader, model, device)"
      ],
      "metadata": {
        "id": "Yi5g8mNGj36p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0989128-63e9-41f3-f759-a024172bc267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Loading checkpoint\n",
            "Dice score:0.7924194109598858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load_model:\n",
        "    load_checkpoint(torch.load(\"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/my_checkpoint_table_10.pth\",map_location=torch.device('cpu')),model)    \n",
        "    #model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/resnet_weight.pth\", map_location=\"cpu\"))\n",
        "for epoch in range(num_epochs):\n",
        "    train(train_loader, model, optimizer, loss_fn)\n",
        "\n",
        "    #save model\n",
        "    checkpoint={\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    save_checkpoint(checkpoint)\n",
        "    #check accuracy\n",
        "    dice_coeff(val_loader, model, device)\n",
        "    #print ex\n",
        "save_predictions_as_imgs(\n",
        "    val_loader, model, folder=\"/content/drive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/save_img_train\",\n",
        "    device=device\n",
        ")\n",
        "%tensorboard --logdir=runs   "
      ],
      "metadata": {
        "id": "sia-z__agNdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b90a99f-5250-43c4-dcd5-267f7c9c5048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Loading checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.89it/s, loss=0.0355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8000049999440781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:49<00:00,  3.81it/s, loss=0.0448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8013131348367308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0205]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7969011772904584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.794164731119331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0242]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8010096407971105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0434]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7883194869918528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:49<00:00,  3.80it/s, loss=0.087]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8038737127211016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0624]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7950514140888955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8027347667522537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8010353422564652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0666]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8025442981531041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.85it/s, loss=0.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.801017244480034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.0374]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7991162311774266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8024167678782038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.801854897218121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7938427091484811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7976658558743859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0278]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.801846540669589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.0283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7997113134290726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0725]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8055929343870711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0849]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8030548957466825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:49<00:00,  3.81it/s, loss=0.0677]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7949141346728217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0369]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7954821091005869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8009775818087149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.796781910725272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.78574828683323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0526]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8028519228388159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:49<00:00,  3.81it/s, loss=0.0507]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7960869836195943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.85it/s, loss=0.124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8009866712546597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0413]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7953844292436834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0464]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7981496134001052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.0692]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8075429961928754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.85it/s, loss=0.127]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8018409705937166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.8015778897033216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.84it/s, loss=0.0338]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7904494062659655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0495]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7926253516182434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7971446356828397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0456]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7931817756404234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7949963982603527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.048]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7977775619987478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:49<00:00,  3.81it/s, loss=0.0832]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7993173771010875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.83it/s, loss=0.0436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7944774562698846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [00:48<00:00,  3.82it/s, loss=0.0559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=>Saving checkpoint\n",
            "Dice score:0.7977960668702428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 47/187 [00:12<00:36,  3.85it/s, loss=0.106]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import gdown\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "Oj59G4uHGsbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2), DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(\n",
        "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
        "            )\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "CFvsU42fEO5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn.modules.batchnorm import BatchNorm2d\n",
        "\n",
        "def conv(ni, nf, ks=3, stride=1, act=True, bn=True):\n",
        "  layers = []\n",
        "  layers.append(\n",
        "      nn.Conv2d(\n",
        "          ni, nf, kernel_size=ks, padding=(ks - 1) // 2, stride=stride, bias=False\n",
        "      )\n",
        "  )\n",
        "  if act:\n",
        "      layers.append(nn.ReLU(inplace=True))\n",
        "  if bn:\n",
        "      layers.append(BatchNorm2d(nf))\n",
        "\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def conv_block(ni, nf, stride):\n",
        "  return nn.Sequential(\n",
        "      conv(ni, nf // 4, ks=1),\n",
        "      conv(nf // 4, nf // 4, stride=stride),\n",
        "      conv(nf // 4, nf, ks=1, bn=True, act=False),\n",
        "  )\n",
        "\n",
        "\n",
        "def _resnet_stem(*sizes):\n",
        "  convs = [\n",
        "      conv(sizes[i], sizes[i + 1], ks=3, stride=2 if i == 0 else 1)\n",
        "      for i in range(len(sizes) - 1)\n",
        "  ]\n",
        "  return nn.Sequential(*convs)\n",
        "\n",
        "\n",
        "def noop(x):\n",
        "  return x\n",
        "\n",
        "\n",
        "def block(ni, nf, idx, stride=2, nblocks=2):\n",
        "  return nn.Sequential(\n",
        "      *[\n",
        "          ResBlock(ni if i == 0 else nf, nf, stride=stride if i == 0 else 1)\n",
        "          for i in range(nblocks)\n",
        "      ]\n",
        "  )\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, ni, nf, stride=1):\n",
        "      super(ResBlock, self).__init__()\n",
        "      self.convs = conv_block(ni, nf, stride)\n",
        "      self.idconv = noop if ni == nf else conv(ni, nf, 1, act=None)\n",
        "      self.pool = noop if stride == 1 else nn.AvgPool2d(2, ceil_mode=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.convs(x) + self.idconv(self.pool(x))\n",
        "\n",
        "\n",
        "class ResUnet(nn.Module):\n",
        "  def __init__(self, n_classes, bilinear=True):\n",
        "      super(ResUnet, self).__init__()\n",
        "      self.n_classes = n_classes\n",
        "      self.bilinear = bilinear\n",
        "\n",
        "      self.stem = _resnet_stem(3, 32, 32, 64)\n",
        "      self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "      self.block1 = block(64, 64, 0, stride=1, nblocks=3)\n",
        "      self.block2 = block(64, 128, 0, stride=2, nblocks=4)\n",
        "      self.block3 = block(128, 256, 0, stride=2, nblocks=6)\n",
        "      self.block4 = block(256, 512, 0, stride=2, nblocks=3)\n",
        "\n",
        "      self.up6 = Up(512 + 256, 256, bilinear)\n",
        "      self.up5 = Up(256 + 128, 128, bilinear)\n",
        "      self.up4 = Up(128 + 64, 64, bilinear)\n",
        "      self.up3 = Up(64 + 64, 64, bilinear)\n",
        "      self.up2 = Up(64 + 64, 32, bilinear)\n",
        "      self.up1 = Up(32 + 3, 32, bilinear)\n",
        "\n",
        "      self.outconv = OutConv(32, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "      d1 = self.stem(x)  # 64\n",
        "      d2 = self.pool(d1)\n",
        "\n",
        "      d3 = self.block1(d2)  # 64\n",
        "      d4 = self.block2(d3)  # 128\n",
        "      d5 = self.block3(d4)  # 256\n",
        "      d6 = self.block4(d5)  # 512\n",
        "\n",
        "      h = self.up6(d6, d5)  # 512 + 256 -> 256\n",
        "      h = self.up5(h, d4)  # 256 + 128 -> 128\n",
        "      h = self.up4(h, d3)  #\n",
        "      h = self.up3(h, d2)\n",
        "      h = self.up2(h, d1)\n",
        "      h = self.up1(h, x)\n",
        "\n",
        "      h = self.outconv(h)\n",
        "      return h"
      ],
      "metadata": {
        "id": "FbZuKTUuDh4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_unet(weight_path: str, device: torch.device) -> torch.nn.Module:\n",
        "    \"\"\"Initialize the line segment model\n",
        "    Args:\n",
        "        weight_path (str): path to weight file\n",
        "        device (torch.device): torch device\n",
        "    Returns:\n",
        "        torch.nn.Module: Unet model\n",
        "    \"\"\"\n",
        "    net = ResUnet(n_classes=1)\n",
        "    net.to(device=device)\n",
        "    # load pretrained weight\n",
        "    net.load_state_dict(torch.load(weight_path, map_location=device))\n",
        "    return net"
      ],
      "metadata": {
        "id": "5PbtAgytOKkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(\n",
        "    img: np.ndarray,\n",
        "    scale_factor: float = 0.25,\n",
        "    out_threshold: float = 0.5,\n",
        "    mask_path: str=None,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Take input as an table image and return a mask of the same size\n",
        "    as the input image and each pixel has a value of 1 if that pixel belongs\n",
        "    to a line otherwise it will be 0.\n",
        "    Args:\n",
        "        img (np.array): table image\n",
        "        scale_factor (float, optional): factor for downscaling original image.\n",
        "        Defaults to 0.5.\n",
        "        out_threshold (float, optional): confidence threshold. Defaults to 0.5.\n",
        "    Returns:\n",
        "        numpy.ndarray: mask image has same size with original image\n",
        "    \"\"\"\n",
        "    h, w, _ = img.shape\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    padding_pil_img, preprocessed_img, pad = _preprocess(\n",
        "        img=img,\n",
        "        scale=scale_factor,\n",
        "    )\n",
        "    print(padding_pil_img.size, preprocessed_img.shape, pad)\n",
        "    padding_pil_img.save('img.jpg')\n",
        "    ts_img = torch.from_numpy(preprocessed_img)\n",
        "    ts_img = ts_img.unsqueeze(0)\n",
        "    ts_img = ts_img.to(device=device, dtype=torch.float32)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(ts_img)\n",
        "        probs = torch.sigmoid(output)\n",
        "        probs = probs.squeeze(0)\n",
        "        tf = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "        probs = tf(probs.cpu())\n",
        "        full_mask = probs.squeeze().cpu().numpy()\n",
        "        mask = full_mask > out_threshold\n",
        "        mask = _normalize(padding_pil_img, mask_img=mask)\n",
        "        mask = np.array(mask[50:-50, 50:-50])\n",
        "        cv2.imwrite('test.jpg', mask * 255)\n",
        "        mask_truth_new = create_mask(img_h=h,img_w=w,label_path=mask_path,LINE_HEIGHT_RATIO=0.5)\n",
        "        mask_truth = _normalize(padding_pil_img, mask_img=mask_truth_new)\n",
        "        mask_truth = np.array(mask_truth[50:-50, 50:-50])\n",
        "        cv2.imwrite('test1.jpg', mask_truth * 255)\n",
        "        smooth = 1e-8\n",
        "        intersection = (mask* mask_truth).sum()\n",
        "        dice_score += ((2.*intersection)+ smooth)/(mask.sum() + mask_truth.sum() + smooth)\n",
        "    print(dice_score)\n",
        "\n",
        "def _preprocess(\n",
        "    img: np.ndarray,\n",
        "    scale: float,\n",
        "    pad: int = 0,\n",
        "):\n",
        "    \"\"\"Add pad to table image from path then resize image\n",
        "    Args:\n",
        "        img (np.array): table image\n",
        "        scale (float): Scale factor\n",
        "        pad (int, optional): Pad to add to image. Defaults to 5.\n",
        "    Returns:\n",
        "        PIL.Image.Image: PIL image for size-recovering purpose\n",
        "        numpy.array: image after preprocessing\n",
        "        int: pad value for size-recovering purpose\n",
        "    \"\"\"\n",
        "    # Padding\n",
        "    h, w, _ = img.shape\n",
        "    assert pad >= 0, \"Pad must great than 0\"\n",
        "    padding_img = np.ones((h + pad * 2, w + pad * 2, 3), dtype=np.uint8) * 255\n",
        "    padding_img[pad : h + pad, pad : w + pad, :] = img\n",
        "    pil_img = Image.fromarray(padding_img)\n",
        "\n",
        "    # Resize\n",
        "    newW, newH = int(scale * (w + pad * 2)), int(scale * (h + pad * 2))\n",
        "    assert newW > 0 and newH > 0, \"Scale is too small\"\n",
        "    rz_pil_img = pil_img.resize((newW, newH))\n",
        "    img_nd = np.array(rz_pil_img)\n",
        "\n",
        "    # HWC to CHW\n",
        "    img_trans = img_nd.transpose((2, 0, 1))\n",
        "    if img_trans.max() > 1:\n",
        "        img_trans = img_trans / 255\n",
        "    return pil_img, img_trans, pad\n",
        "\n",
        "def _normalize(img: Image.Image, mask_img: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Convert shape of mask image to shape of img\n",
        "    Args:\n",
        "        img (PIL.Image.Image): original table image (H, W, C)\n",
        "        mask_img (numpy.ndarray): binary image of original table image (H1, W1, C1)\n",
        "    Returns:\n",
        "        numpy.ndarray: Mask image has shape of (H, W, C)\n",
        "    \"\"\"\n",
        "    mask = np.asarray(mask_img)\n",
        "    img = np.asarray(img)\n",
        "    img_h, img_w = img.shape[:2]\n",
        "    mask = mask.reshape(mask.shape[0], mask.shape[1])\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask = cv2.resize(mask, (img_w, img_h), cv2.INTER_AREA)\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "u6IM-lXbG9zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model_unet(weight_path=\"resnet_weight.pth\",device=\"cuda\")"
      ],
      "metadata": {
        "id": "EKygcHdnGSlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/resnet_weight.pth\"\n",
        "WEIGHT_URL = \"https://drive.google.com/u/0/uc?id=18YEiAzUs9NXz0FwBuU0JicEWc_F2V7tq\""
      ],
      "metadata": {
        "id": "I_DVPuwLEQzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "UXGbUvwfAHHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "num_epochs = 1\n",
        "#model = load_model_unet(weight_path=\"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/resnet_weight.pth\",device=\"cuda\")\n",
        "\n",
        "load_checkpoint(torch.load(\"/content/gdrive/MyDrive/Báo cáo thực tập - Nguyễn Hữu Khải/Segmentation/my_checkpoint_table_repo_10.pth\",map_location=torch.device('cuda')),model)       \n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(num_epochs):\n",
        "    train(train_loader, model, optimizer, loss_fn)\n",
        "    dice_coeff(val_loader, model, device)\n",
        "    checkpoint={\n",
        "        \"state_dict\": model.state_dict(),\n",
        "    }\n",
        "    save_checkpoint(checkpoint)"
      ],
      "metadata": {
        "id": "0F9wlLWh-RMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "for img_path in glob.glob(\"img_train/*.png\"):\n",
        "    print(img_path)\n",
        "    image = np.array(cv2.imread(img_path))\n",
        "    cv2.imwrite('test_img.jpg', image)\n",
        "    label_path = img_path.replace('img', 'label').replace('png','txt')\n",
        "    predict(img=image,mask_path=label_path)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "GsiRNr51Lj6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}